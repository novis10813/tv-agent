"""
Agentic TV Controller
Webhook service that uses LiteLLM to control Android TV via MCP
"""

import json
import os
from contextlib import asynccontextmanager

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from openai import AsyncOpenAI
from pydantic import BaseModel

from mcp_client import MCPClient

# Load environment variables
load_dotenv()

# Configuration
LITELLM_BASE_URL = os.getenv("LITELLM_BASE_URL", "http://litellm.homelab.com")
LITELLM_API_KEY = os.getenv("LITELLM_API_KEY", "")
LITELLM_MODEL = os.getenv("LITELLM_MODEL", "llama-3.1-405b-instruct")
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "http://192.168.0.13:8765/mcp")
HOST = os.getenv("HOST", "0.0.0.0")
PORT = int(os.getenv("PORT", "8000"))

# Global clients
mcp_client: MCPClient | None = None
openai_client: AsyncOpenAI | None = None
openai_tools: list[dict] = []


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize clients on startup"""
    global mcp_client, openai_client, openai_tools
    
    print(f"ğŸš€ Agentic TV Controller")
    print(f"   LiteLLM: {LITELLM_BASE_URL}")
    print(f"   Model: {LITELLM_MODEL}")
    print(f"   MCP Server: {MCP_SERVER_URL}")
    
    # Initialize MCP client
    mcp_client = MCPClient(MCP_SERVER_URL)
    
    try:
        # Initialize MCP session
        await mcp_client.initialize()
        print("   âœ“ MCP connection established")
        
        # Get available tools
        mcp_tools = await mcp_client.list_tools()
        print(f"   âœ“ {len(mcp_tools)} tools available")
        
        # Convert to OpenAI format
        openai_tools = mcp_client.mcp_tools_to_openai_tools(mcp_tools)
        
    except Exception as e:
        print(f"   âœ— MCP connection failed: {e}")
        print("   Server will start but TV control may not work")
    
    # Initialize OpenAI client (pointing to LiteLLM)
    openai_client = AsyncOpenAI(
        base_url=LITELLM_BASE_URL,
        api_key=LITELLM_API_KEY or "dummy-key"
    )
    
    yield
    
    print("Shutting down...")


app = FastAPI(
    title="Agentic TV Controller",
    description="Control your Android TV with natural language",
    lifespan=lifespan
)


class CommandRequest(BaseModel):
    text: str


class CommandResponse(BaseModel):
    success: bool
    message: str
    tool_calls: list[dict] = []


SYSTEM_PROMPT = """ä½ æ˜¯ä¸€å€‹é›»è¦–æ§åˆ¶åŠ©æ‰‹ã€‚ç”¨æˆ¶æœƒçµ¦ä½ è‡ªç„¶èªè¨€æŒ‡ä»¤ï¼Œä½ éœ€è¦ä½¿ç”¨æä¾›çš„ tools ä¾†æ§åˆ¶é›»è¦–ã€‚

å¸¸è¦‹æŒ‡ä»¤å°æ‡‰ï¼š
- "æ‰“é–‹ YouTube" â†’ youtube_launch
- "æœå°‹ XXX" â†’ youtube_search æˆ– netflix_search
- "æš«åœ" â†’ play_pause
- "å€’é€€ 10 ç§’" â†’ rewind(app, seconds)
- "å¿«è½‰" â†’ fast_forward(app, seconds)
- "éŸ³é‡èª¿å¤§/å°" â†’ tv_volume
- "å›é¦–é " â†’ tv_remote(key="home")
- "åˆ‡æ›åˆ° HDMI 1" â†’ tv_input_source(hdmi=1)

æ³¨æ„ï¼š
- ä½¿ç”¨ rewind/fast_forward æ™‚ï¼Œéœ€è¦æŒ‡å®š app æ˜¯ "youtube" é‚„æ˜¯ "netflix"
- å¦‚æœä¸ç¢ºå®šç•¶å‰æ˜¯ä»€éº¼ Appï¼Œå¯ä»¥ç”¨ tv_current_app æŸ¥è©¢

è«‹ç›´æ¥åŸ·è¡Œæ“ä½œï¼Œä¸éœ€è¦å¤šé¤˜çš„è§£é‡‹ã€‚"""


@app.post("/command", response_model=CommandResponse)
async def handle_command(request: CommandRequest):
    """
    è™•ç†è‡ªç„¶èªè¨€æŒ‡ä»¤ï¼Œé€é LLM é¸æ“‡ä¸¦åŸ·è¡Œé©ç•¶çš„ TV æ§åˆ¶æ“ä½œ
    """
    global mcp_client, openai_client, openai_tools
    
    if not mcp_client or not openai_client:
        raise HTTPException(status_code=503, detail="Service not initialized")
    
    if not openai_tools:
        # Try to refresh tools
        try:
            mcp_tools = await mcp_client.list_tools()
            openai_tools = mcp_client.mcp_tools_to_openai_tools(mcp_tools)
        except Exception as e:
            raise HTTPException(status_code=503, detail=f"Cannot get tools: {e}")
    
    try:
        # Call LLM with tools
        response = await openai_client.chat.completions.create(
            model=LITELLM_MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": request.text}
            ],
            tools=openai_tools,
            tool_choice="auto"
        )
        
        message = response.choices[0].message
        tool_results = []
        
        # Process tool calls
        if message.tool_calls:
            for tool_call in message.tool_calls:
                tool_name = tool_call.function.name
                tool_args = json.loads(tool_call.function.arguments)
                
                # Execute tool via MCP
                result = await mcp_client.call_tool(tool_name, tool_args)
                
                tool_results.append({
                    "tool": tool_name,
                    "args": tool_args,
                    "result": result
                })
        
        # Generate response message
        if tool_results:
            result_messages = [r["result"] for r in tool_results]
            final_message = " | ".join(result_messages)
        else:
            final_message = message.content or "æ²’æœ‰åŸ·è¡Œä»»ä½•æ“ä½œ"
        
        return CommandResponse(
            success=True,
            message=final_message,
            tool_calls=tool_results
        )
        
    except Exception as e:
        return CommandResponse(
            success=False,
            message=f"Error: {str(e)}",
            tool_calls=[]
        )


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "ok",
        "mcp_connected": mcp_client is not None,
        "tools_count": len(openai_tools)
    }


@app.get("/tools")
async def list_tools():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ tools"""
    return {
        "tools": [t["function"]["name"] for t in openai_tools]
    }


def main():
    """Run the server"""
    import uvicorn
    uvicorn.run(app, host=HOST, port=PORT)


if __name__ == "__main__":
    main()
